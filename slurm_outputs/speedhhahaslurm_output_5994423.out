============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
wandb: Currently logged in as: ge_vit_DL2 (use `wandb login --relogin` to force relogin)
activation_function: Swish
attention_type: Local
augment: false
batch_size: 16
comment: ''
data_fraction: 1.0
dataset: rotMNIST
device: cuda
dropout_att: 0.1
dropout_values: 0.1
epochs: 50
lr: 0.001
model: p4msa
norm_type: LayerNorm
only_3_and_8: true
optimizer: Adam
optimizer_momentum: 0.9
patch_size: 5
path: ''
pretrained: false
sched_decay_factor: 1.0
sched_decay_steps: !!python/tuple
- 1000
scheduler: constants
seed: 0
train: true
weight_decay: 0.0001
whitening_scale: 1.41421356

wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.18
wandb: Syncing run crimson-morning-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ge_vit_DL2/rotMNIST_3_and_8_p4msa
wandb: üöÄ View run at https://wandb.ai/ge_vit_DL2/rotMNIST_3_and_8_p4msa/runs/ni56k9z1
wandb: Run data is saved locally in /gpfs/home5/scur0390/only3and8/GEVit-DL2-Project/wandb/run-20240421_142553-ni56k9z1
wandb: Run `wandb offline` to turn off syncing.
GPU's available: 1
Number of parameters: 44348

------------------------------
Running experiment on rotmnist dataset
Number of train samples: 1965
Number of validation samples: 398
Number of test samples: 9972

Label distribution in training set:
Label 3: 1026 samples
Label 8: 939 samples

Label distribution in validation set:
Label 3: 189 samples
Label 8: 209 samples

Label distribution in test set:
Label 3: 5108 samples
Label 8: 4864 samples

------------------------------
activation_function: Swish
attention_type: Local
augment: false
batch_size: 16
comment: ''
data_fraction: 1.0
dataset: rotMNIST
device: cuda:0
dropout_att: 0.1
dropout_values: 0.1
epochs: 50
lr: 0.001
model: p4msa
norm_type: LayerNorm
only_3_and_8: true
optimizer: Adam
optimizer_momentum: 0.9
patch_size: 5
path: saved/rotMNIST_model_p4msa_type_Local_patch_5_dpatt_0.1_dpval_0.1_activ_Swish_norm_LayerNorm_white_1.41421356_optim_Adam_lr_0.001_bs_16_ep_50_wd_0.0001_seed_0_sched_constants_schdec_1.0.pt
pretrained: false
sched_decay_factor: 1.0
sched_decay_steps: !!python/tuple
- 1000
scheduler: constants
seed: 0
train: true
weight_decay: 0.0001
whitening_scale: 1.41421356

2024-04-21 14:26:02.122702
  0%|          | 0/50 [00:00<?, ?it/s]Epoch 1/50
------------------------------
Learning Rate: 0.001
------------------------------
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.
/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.
  0%|          | 0/50 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "run_experiment.py", line 85, in <module>
    app.run(main)
  File "/home/scur0390/.conda/envs/g_selfatt/lib/python3.7/site-packages/absl/app.py", line 303, in run
    _run_main(main, args)
  File "/home/scur0390/.conda/envs/g_selfatt/lib/python3.7/site-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "run_experiment.py", line 78, in main
    trainer.train(model, dataloaders, config)
  File "/gpfs/home5/scur0390/only3and8/GEVit-DL2-Project/trainer.py", line 99, in train
    if loss.item() != loss.item(): # loss is nan. FIXME: this should not be allowed to happen!!
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
wandb: Waiting for W&B process to finish, PID 3655184
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gpfs/home5/scur0390/only3and8/GEVit-DL2-Project/wandb/run-20240421_142553-ni56k9z1/logs/debug.log
wandb: Find internal logs for this run at: /gpfs/home5/scur0390/only3and8/GEVit-DL2-Project/wandb/run-20240421_142553-ni56k9z1/logs/debug-internal.log
wandb: Run summary:
wandb:    no_params 44348
wandb:           lr 0.001
wandb:     _runtime 9
wandb:   _timestamp 1713702362
wandb:        _step 1
wandb: Run history:
wandb:           lr ‚ñÅ
wandb:     _runtime ‚ñÅ
wandb:   _timestamp ‚ñÅ
wandb:        _step ‚ñÅ
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced crimson-morning-4: https://wandb.ai/ge_vit_DL2/rotMNIST_3_and_8_p4msa/runs/ni56k9z1


JOB STATISTICS
==============
Job ID: 5994423
Cluster: snellius
User/Group: scur0390/scur0390
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 00:08:06 core-walltime
Job Wall-clock time: 00:00:27
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
