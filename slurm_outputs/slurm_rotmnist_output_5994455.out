============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
wandb: Currently logged in as: ge_vit_DL2 (use `wandb login --relogin` to force relogin)
activation_function: Swish
attention_type: Local
augment: false
batch_size: 16
comment: ''
data_fraction: 1.0
dataset: rotMNIST
device: cuda
dropout_att: 0.1
dropout_values: 0.1
epochs: 50
lr: 0.001
model: p4msa
norm_type: LayerNorm
only_3_and_8: true
optimizer: Adam
optimizer_momentum: 0.9
patch_size: 5
path: ''
pretrained: false
sched_decay_factor: 1.0
sched_decay_steps: !!python/tuple
- 1000
scheduler: constants
seed: 0
train: true
weight_decay: 0.0001
whitening_scale: 1.41421356

wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.18
wandb: Syncing run clear-butterfly-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ge_vit_DL2/rotMNIST_3_and_8_p4msa
wandb: üöÄ View run at https://wandb.ai/ge_vit_DL2/rotMNIST_3_and_8_p4msa/runs/24cvqxg3
wandb: Run data is saved locally in /gpfs/home5/scur0390/only3and8/GEVit-DL2-Project/wandb/run-20240421_143857-24cvqxg3
wandb: Run `wandb offline` to turn off syncing.
GPU's available: 1
Number of parameters: 44348

------------------------------
Running experiment on rotmnist dataset
Number of train samples: 1965
Number of validation samples: 398
Number of test samples: 9972

Label distribution in training set:
Label 0: 1026 samples
Label 1: 939 samples

Label distribution in validation set:
Label 0: 189 samples
Label 1: 209 samples

Label distribution in test set:
Label 0: 5108 samples
Label 1: 4864 samples

------------------------------
activation_function: Swish
attention_type: Local
augment: false
batch_size: 16
comment: ''
data_fraction: 1.0
dataset: rotMNIST
device: cuda:0
dropout_att: 0.1
dropout_values: 0.1
epochs: 50
lr: 0.001
model: p4msa
norm_type: LayerNorm
only_3_and_8: true
optimizer: Adam
optimizer_momentum: 0.9
patch_size: 5
path: saved/rotMNIST_model_p4msa_type_Local_patch_5_dpatt_0.1_dpval_0.1_activ_Swish_norm_LayerNorm_white_1.41421356_optim_Adam_lr_0.001_bs_16_ep_50_wd_0.0001_seed_0_sched_constants_schdec_1.0.pt
pretrained: false
sched_decay_factor: 1.0
sched_decay_steps: !!python/tuple
- 1000
scheduler: constants
seed: 0
train: true
weight_decay: 0.0001
whitening_scale: 1.41421356

2024-04-21 14:39:05.599352
  0%|          | 0/50 [00:00<?, ?it/s]Epoch 1/50
------------------------------
Learning Rate: 0.001
------------------------------
/home/scur0390/.conda/envs/g_selfatt/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
train Loss: 12.5349 Acc: 0.6809
2024-04-21 14:40:04.337986
validation Loss: 3.8920 Acc: 0.5302
2024-04-21 14:40:09.437569
  2%|‚ñè         | 1/50 [01:03<52:07, 63.83s/it]Epoch 2/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.7905 Acc: 0.8244
2024-04-21 14:41:05.990547
validation Loss: 0.4417 Acc: 0.8668
2024-04-21 14:41:10.913342
  4%|‚ñç         | 2/50 [02:05<50:30, 63.13s/it]Epoch 3/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.6400 Acc: 0.8595
2024-04-21 14:42:07.716042
validation Loss: 0.7246 Acc: 0.8241  6%|‚ñå         | 3/50 [03:07<49:07, 62.72s/it]
2024-04-21 14:42:12.699043
Epoch 4/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.5377 Acc: 0.8712
2024-04-21 14:43:08.268811
validation Loss: 0.3348 Acc: 0.8869
2024-04-21 14:43:13.173886
  8%|‚ñä         | 4/50 [04:07<47:34, 62.06s/it]Epoch 5/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.4082 Acc: 0.8972
2024-04-21 14:44:09.960400
validation Loss: 0.4578 Acc: 0.8970
2024-04-21 14:44:14.917376
 10%|‚ñà         | 5/50 [05:09<46:28, 61.96s/it]Epoch 6/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.3594 Acc: 0.8987
2024-04-21 14:45:11.488570
validation Loss: 1.0388 Acc: 0.7412 12%|‚ñà‚ñè        | 6/50 [06:10<45:20, 61.82s/it]
2024-04-21 14:45:16.442848
Epoch 7/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.3793 Acc: 0.9013
2024-04-21 14:46:12.006475
validation Loss: 0.2378 Acc: 0.9422
2024-04-21 14:46:16.932318
 14%|‚ñà‚ñç        | 7/50 [07:11<44:01, 61.43s/it]Epoch 8/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.3934 Acc: 0.9079
2024-04-21 14:47:13.612406
validation Loss: 0.3896 Acc: 0.8995 16%|‚ñà‚ñå        | 8/50 [08:12<43:02, 61.48s/it]
2024-04-21 14:47:18.562833
Epoch 9/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.3163 Acc: 0.9155
2024-04-21 14:48:14.234439
validation Loss: 0.1802 Acc: 0.9447
2024-04-21 14:48:19.189022
 18%|‚ñà‚ñä        | 9/50 [09:13<41:50, 61.24s/it]Epoch 10/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.4068 Acc: 0.9120
2024-04-21 14:49:16.009139
validation Loss: 0.5629 Acc: 0.8945 20%|‚ñà‚ñà        | 10/50 [10:15<40:55, 61.39s/it]
2024-04-21 14:49:20.969090
Epoch 11/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.2577 Acc: 0.9323
2024-04-21 14:50:16.494828
validation Loss: 0.1594 Acc: 0.9472
2024-04-21 14:50:21.452937
 22%|‚ñà‚ñà‚ñè       | 11/50 [11:15<39:44, 61.13s/it]Epoch 12/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.2383 Acc: 0.9338
2024-04-21 14:51:18.278730
validation Loss: 0.1345 Acc: 0.9623
2024-04-21 14:51:23.268920
 24%|‚ñà‚ñà‚ñç       | 12/50 [12:17<38:50, 61.33s/it]Epoch 13/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.2218 Acc: 0.9384
2024-04-21 14:52:19.881237
validation Loss: 0.1255 Acc: 0.9623
2024-04-21 14:52:24.821292
 26%|‚ñà‚ñà‚ñå       | 13/50 [13:19<37:51, 61.40s/it]Epoch 14/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.2511 Acc: 0.9384
2024-04-21 14:53:21.401065
validation Loss: 0.4860 Acc: 0.9121 28%|‚ñà‚ñà‚ñä       | 14/50 [14:20<36:51, 61.43s/it]
2024-04-21 14:53:26.376956
Epoch 15/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.2920 Acc: 0.9277
2024-04-21 14:54:22.002637
validation Loss: 0.1603 Acc: 0.9623 30%|‚ñà‚ñà‚ñà       | 15/50 [15:21<35:40, 61.16s/it]
2024-04-21 14:54:26.908636
Epoch 16/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.1528 Acc: 0.9557
2024-04-21 14:55:22.459336
validation Loss: 0.1043 Acc: 0.9724
2024-04-21 14:55:27.374597
 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [16:21<34:32, 60.97s/it]Epoch 17/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.1289 Acc: 0.9588
2024-04-21 14:56:24.138306
validation Loss: 0.1463 Acc: 0.9673 34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [17:23<33:39, 61.18s/it]
2024-04-21 14:56:29.112669
Epoch 18/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.2039 Acc: 0.9532
2024-04-21 14:57:24.706676
validation Loss: 0.1411 Acc: 0.9573 36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [18:24<32:31, 60.99s/it]
2024-04-21 14:57:29.639293
Epoch 19/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.1213 Acc: 0.9659
2024-04-21 14:58:25.200602
validation Loss: 0.1809 Acc: 0.9598 38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [19:24<31:26, 60.85s/it]
2024-04-21 14:58:30.172092
Epoch 20/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.1095 Acc: 0.9583
2024-04-21 14:59:25.765036
validation Loss: 0.1260 Acc: 0.9774
2024-04-21 14:59:30.728594
 40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [20:25<30:23, 60.78s/it]Epoch 21/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.1168 Acc: 0.9659
2024-04-21 15:00:27.393266
validation Loss: 0.1240 Acc: 0.9673 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [21:26<29:29, 61.02s/it]
2024-04-21 15:00:32.375568
Epoch 22/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0907 Acc: 0.9695
2024-04-21 15:01:27.940470
validation Loss: 0.1241 Acc: 0.9724 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [22:27<28:24, 60.87s/it]
2024-04-21 15:01:32.887857
Epoch 23/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0908 Acc: 0.9740
2024-04-21 15:02:28.426953
validation Loss: 0.1227 Acc: 0.9724 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [23:27<27:20, 60.76s/it]
2024-04-21 15:02:33.373912
Epoch 24/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0568 Acc: 0.9817
2024-04-21 15:03:28.987989
validation Loss: 0.1684 Acc: 0.9573 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [24:28<26:18, 60.69s/it]
2024-04-21 15:03:33.921144
Epoch 25/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0710 Acc: 0.9781
2024-04-21 15:04:29.512592
validation Loss: 0.1340 Acc: 0.9774 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [25:28<25:16, 60.65s/it]
2024-04-21 15:04:34.468856
Epoch 26/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0735 Acc: 0.9802
2024-04-21 15:05:30.015636
validation Loss: 0.1578 Acc: 0.9598 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [26:29<24:14, 60.61s/it]
2024-04-21 15:05:34.982113
Epoch 27/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0788 Acc: 0.9781
2024-04-21 15:06:30.538749
validation Loss: 0.1406 Acc: 0.9623 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [27:29<23:13, 60.58s/it]
2024-04-21 15:06:35.495595
Epoch 28/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.1381 Acc: 0.9634
2024-04-21 15:07:31.053330
validation Loss: 0.1299 Acc: 0.9724 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [28:30<22:12, 60.57s/it]
2024-04-21 15:07:36.049089
Epoch 29/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0843 Acc: 0.9746
2024-04-21 15:08:31.608816
validation Loss: 0.2943 Acc: 0.9548 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [29:30<21:11, 60.55s/it]
2024-04-21 15:08:36.564146
Epoch 30/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0707 Acc: 0.9735
2024-04-21 15:09:32.171116
validation Loss: 0.1432 Acc: 0.9673 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [30:31<20:11, 60.56s/it]
2024-04-21 15:09:37.141059
Epoch 31/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0655 Acc: 0.9817
2024-04-21 15:10:32.716541
validation Loss: 0.2374 Acc: 0.9598 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [31:32<19:10, 60.56s/it]
2024-04-21 15:10:37.707424
Epoch 32/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.1340 Acc: 0.9613
2024-04-21 15:11:33.322558
validation Loss: 0.1524 Acc: 0.9598 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [32:32<18:10, 60.56s/it]
2024-04-21 15:11:38.258857
Epoch 33/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0923 Acc: 0.9751
2024-04-21 15:12:33.838896
validation Loss: 0.1738 Acc: 0.9724 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [33:33<17:09, 60.55s/it]
2024-04-21 15:12:38.776971
Epoch 34/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.1619 Acc: 0.9588
2024-04-21 15:13:34.336614
validation Loss: 0.1549 Acc: 0.9724 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [34:33<16:08, 60.54s/it]
2024-04-21 15:13:39.298254
Epoch 35/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0869 Acc: 0.9756
2024-04-21 15:14:34.849979
validation Loss: 0.2574 Acc: 0.9472 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [35:34<15:07, 60.53s/it]
2024-04-21 15:14:39.808542
Epoch 36/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0780 Acc: 0.9720
2024-04-21 15:15:35.469165
validation Loss: 0.1730 Acc: 0.9749 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [36:34<14:07, 60.56s/it]
2024-04-21 15:15:40.426018
Epoch 37/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0476 Acc: 0.9832
2024-04-21 15:16:35.988620
validation Loss: 0.1481 Acc: 0.9724 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [37:35<13:07, 60.55s/it]
2024-04-21 15:16:40.955904
Epoch 38/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0524 Acc: 0.9832
2024-04-21 15:17:36.526657
validation Loss: 0.2159 Acc: 0.9673 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [38:35<12:06, 60.54s/it]
2024-04-21 15:17:41.468543
Epoch 39/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0935 Acc: 0.9771
2024-04-21 15:18:37.050157
validation Loss: 0.1655 Acc: 0.9698 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [39:36<11:05, 60.54s/it]
2024-04-21 15:18:42.000365
Epoch 40/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0575 Acc: 0.9837
2024-04-21 15:19:37.653671
validation Loss: 0.1570 Acc: 0.9698 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [40:36<10:05, 60.56s/it]
2024-04-21 15:19:42.601266
Epoch 41/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0834 Acc: 0.9746
2024-04-21 15:20:38.231690
validation Loss: 0.1447 Acc: 0.9799
2024-04-21 15:20:43.212419
 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [41:37<09:05, 60.59s/it]Epoch 42/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0672 Acc: 0.9781
2024-04-21 15:21:39.790819
validation Loss: 0.1513 Acc: 0.9724 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [42:39<08:06, 60.85s/it]
2024-04-21 15:21:44.717855
Epoch 43/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0688 Acc: 0.9791
2024-04-21 15:22:40.296452
validation Loss: 0.1434 Acc: 0.9824
2024-04-21 15:22:45.273384
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [43:39<07:05, 60.77s/it]Epoch 44/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0450 Acc: 0.9842
2024-04-21 15:23:42.043845
validation Loss: 0.1761 Acc: 0.9724 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [44:41<06:06, 61.04s/it]
2024-04-21 15:23:46.980751
Epoch 45/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0736 Acc: 0.9796
2024-04-21 15:24:42.542907
validation Loss: 0.1084 Acc: 0.9799 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [45:41<05:04, 60.89s/it]
2024-04-21 15:24:47.506492
Epoch 46/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0751 Acc: 0.9796
2024-04-21 15:25:43.098467
validation Loss: 0.1003 Acc: 0.9874
2024-04-21 15:25:48.041554
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [46:42<04:03, 60.79s/it]Epoch 47/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0670 Acc: 0.9802
2024-04-21 15:26:44.801432
validation Loss: 0.1383 Acc: 0.9724 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [47:44<03:03, 61.05s/it]
2024-04-21 15:26:49.748595
Epoch 48/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0339 Acc: 0.9903
2024-04-21 15:27:45.265736
validation Loss: 0.2226 Acc: 0.9598 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [48:44<02:01, 60.88s/it]
2024-04-21 15:27:50.214899
Epoch 49/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0390 Acc: 0.9873
2024-04-21 15:28:45.793756
validation Loss: 0.1867 Acc: 0.9724 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [49:45<01:00, 60.77s/it]
2024-04-21 15:28:50.735030
Epoch 50/50
------------------------------
Learning Rate: 0.001
------------------------------
train Loss: 0.0367 Acc: 0.9863
2024-04-21 15:29:46.288951
validation Loss: 0.1844 Acc: 0.9724100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [50:45<00:00, 60.69s/it]
2024-04-21 15:29:51.223383
Best Val Acc: 0.9874
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [50:45<00:00, 60.91s/it]
Accuracy of the network on the 9972 test images: 98.14480545527476
wandb: Waiting for W&B process to finish, PID 1704003
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.57MB of 0.57MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gpfs/home5/scur0390/only3and8/GEVit-DL2-Project/wandb/run-20240421_143857-24cvqxg3/logs/debug.log
wandb: Find internal logs for this run at: /gpfs/home5/scur0390/only3and8/GEVit-DL2-Project/wandb/run-20240421_143857-24cvqxg3/logs/debug-internal.log
wandb: Run summary:
wandb:             no_params 44348
wandb:     best_val_accuracy 0.98744
wandb:         best_val_loss 0.10028
wandb:    best_test_accuracy 0.98744
wandb:                    lr 0.001
wandb:        accuracy_train 0.98626
wandb:            loss_train 0.03671
wandb:   accuracy_validation 0.97236
wandb:       loss_validation 0.18439
wandb:         accuracy_test 0.98744
wandb:              _runtime 3167
wandb:            _timestamp 1713706304
wandb:                 _step 50
wandb: Run history:
wandb:                    lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        accuracy_train ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:            loss_train ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   accuracy_validation ‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       loss_validation ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         accuracy_test ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:              _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:            _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                 _step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: 
wandb: Synced clear-butterfly-5: https://wandb.ai/ge_vit_DL2/rotMNIST_3_and_8_p4msa/runs/24cvqxg3


JOB STATISTICS
==============
Job ID: 5994455
Cluster: snellius
User/Group: scur0390/scur0390
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 15:55:48 core-walltime
Job Wall-clock time: 00:53:06
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
