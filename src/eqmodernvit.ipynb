{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tvtf\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from datasets import MNIST_rot\n",
    "from train_vit import VisionTransformer\n",
    "\n",
    "import models\n",
    "import g_selfatt.groups as groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = (0.1307,)\n",
    "data_stddev = (0.3081,)\n",
    "\n",
    "transform_train = tvtf.Compose([\n",
    "    tvtf.RandomRotation(degrees=(-180, 180)),  # random rotation\n",
    "    tvtf.ToTensor(),\n",
    "    tvtf.Normalize(data_mean, data_stddev)\n",
    "])\n",
    "transform_test = tvtf.Compose(\n",
    "    [\n",
    "        tvtf.ToTensor(),\n",
    "        tvtf.Normalize(data_mean, data_stddev),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = MNIST_rot(root=\"../data\", stage=\"train\", download=True, transform=transform_train, data_fraction=1, only_3_and_8=False)\n",
    "validation_set = MNIST_rot(root=\"../data\", stage=\"validation\", download=True, transform=transform_test, data_fraction=1, only_3_and_8=False)\n",
    "test_set = MNIST_rot(root=\"../data\", stage=\"test\", download=True, transform=transform_test, data_fraction=1, only_3_and_8=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "img_loader = torch.utils.data.DataLoader(  # single element for visualization purposes\n",
    "    test_set,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/context.py:281\u001b[0m, in \u001b[0;36mForkProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_fork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/popen_fork.py:66\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m parent_r, child_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[1;32m     65\u001b[0m child_r, parent_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "data, target = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(images, n_rotations=4, flips=True):\n",
    "    \"\"\" Returns all transformations of the input images \"\"\"\n",
    "\n",
    "    B, C, H, W = images.shape\n",
    "    T = 2*n_rotations if flips else n_rotations  # number of transformations\n",
    "\n",
    "    # initialize empty transforms tensor\n",
    "    transforms = torch.empty(size=(B, T, C, H, W))\n",
    "    transforms[:, 0,...] = images\n",
    "    idx = 1\n",
    "\n",
    "    # remember all orientations that need to be flipped\n",
    "    orientations = [images] if flips else []\n",
    "\n",
    "    # rotations\n",
    "    for i in range(1, n_rotations):\n",
    "        angle = i * (360 / n_rotations)\n",
    "        rotated_images = TF.rotate(images, angle)  # B, C, H, W\n",
    "        transforms[:, idx,...] = rotated_images\n",
    "        idx += 1\n",
    "\n",
    "        if flips:\n",
    "            orientations.append(rotated_images)\n",
    "\n",
    "    # flips\n",
    "    for transform in orientations:\n",
    "        flipped_image = TF.hflip(transform)\n",
    "        transforms[:, idx, ...] = flipped_image\n",
    "        idx += 1\n",
    "\n",
    "    return transforms  # B, T, C, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size: Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels: If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H // patch_size, patch_size, W // patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)  # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1, 2)  # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2, 4)  # [B, H'*W', C*p_H*p_W]\n",
    "    return x\n",
    "\n",
    "\n",
    "class EquivariantViT(nn.Module):\n",
    "    def __init__(self, patch_size=7, num_patches=16, num_channels=1, n_rotations=4, flips=True, n_embd=1):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels  \n",
    "        self.n_rotations = n_rotations\n",
    "        self.flips = flips\n",
    "        self.n_embd = n_embd\n",
    "        self.num_patches_x = int(math.sqrt(num_patches))\n",
    "        # below can be more intricate, but for now we just use a linear layer\n",
    "        self.project = nn.Linear(num_channels*patch_size**2, n_embd)  # to project the patches to their embedding space\n",
    "        self.gevit = models.GroupTransformer(\n",
    "            group=groups.SE2(num_elements=8),\n",
    "            in_channels=1,\n",
    "            num_channels=20,\n",
    "            block_sizes=[2, 3],\n",
    "            expansion_per_block=1,\n",
    "            crop_per_layer=[0, 0, 0, 0, 0],\n",
    "            image_size=self.num_patches_x,\n",
    "            num_classes=10,\n",
    "            dropout_rate_after_maxpooling=0.0,\n",
    "            maxpool_after_last_block=False,\n",
    "            normalize_between_layers=False,\n",
    "            patch_size=None,\n",
    "            num_heads=9,\n",
    "            norm_type=\"LayerNorm\",\n",
    "            activation_function=\"Swish\",\n",
    "            attention_dropout_rate=0.0,\n",
    "            value_dropout_rate=0.01,\n",
    "            whitening_scale=1.41421356,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the patches\n",
    "        x = img_to_patch(x, self.patch_size, flatten_channels=False)  # B, num_patches, C, patch_size, patch_size\n",
    "        B, num_patches, C, patch_size, _ = x.shape\n",
    "\n",
    "        # get all transformations for the patches\n",
    "        x = x.view(B*num_patches, C, patch_size, patch_size)\n",
    "        x = get_transforms(x, n_rotations=self.n_rotations, flips=self.flips)\n",
    "\n",
    "        T = x.shape[1]  # number of transformations\n",
    "\n",
    "        # flatten and project all patches\n",
    "        x = x.view(B*num_patches*T, C*patch_size*patch_size)\n",
    "        x = self.project(x)\n",
    "\n",
    "        # combine the transformations for the patches to make it invariant\n",
    "        x = x.view(B, num_patches, T, self.n_embd)  # TODO check this\n",
    "        x = x.mean(dim=2)\n",
    "        \n",
    "        # reshape to image grid\n",
    "        x = x.view(B, self.num_patches_x, self.num_patches_x, self.n_embd).permute(0, 3, 1, 2)\n",
    "\n",
    "        # print(x.shape)\n",
    "        # pass through the GEViT to get predictions\n",
    "        x = self.gevit(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.9188,  -1.2069,  12.8977, -12.0133,  26.3354,  13.2836,   5.6998,\n",
       "          19.4729,  -2.1033,  -3.6100],\n",
       "        [  0.9188,  -1.2069,  12.8978, -12.0133,  26.3354,  13.2837,   5.6998,\n",
       "          19.4729,  -2.1033,  -3.6100],\n",
       "        [  0.9188,  -1.2069,  12.8978, -12.0133,  26.3354,  13.2837,   5.6998,\n",
       "          19.4729,  -2.1033,  -3.6100],\n",
       "        [  0.9188,  -1.2069,  12.8977, -12.0133,  26.3354,  13.2836,   5.6998,\n",
       "          19.4729,  -2.1033,  -3.6100]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = 4\n",
    "gevit = models.GroupTransformer(\n",
    "    group=groups.SE2(num_elements=8),\n",
    "    in_channels=1,\n",
    "    num_channels=20,\n",
    "    block_sizes=[2, 3],\n",
    "    expansion_per_block=1,\n",
    "    crop_per_layer=[0, 0, 0, 0, 0],\n",
    "    image_size=ms,\n",
    "    num_classes=10,\n",
    "    dropout_rate_after_maxpooling=0.0,\n",
    "    maxpool_after_last_block=False,\n",
    "    normalize_between_layers=False,\n",
    "    patch_size=3,\n",
    "    num_heads=9,\n",
    "    norm_type=\"LayerNorm\",\n",
    "    activation_function=\"Swish\",\n",
    "    attention_dropout_rate=0.0,\n",
    "    value_dropout_rate=0.01,\n",
    "    whitening_scale=1.41421356,\n",
    ")\n",
    "gevit.eval()\n",
    "gevit(get_transforms(torch.randn((1, 1, ms, ms)).float(), n_rotations=4, flips=False).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 1, 28, 28]), torch.Size([1, 1, 28, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data = get_transforms(data, n_rotations=4, flips=False)\n",
    "n_data.shape, data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 32.9556,  47.2671,  46.2578,  10.6949,  32.2529,  63.4244, -36.7843,\n",
       "          42.2023, -32.5435,   6.3517],\n",
       "        [ 38.2196,  47.0096,  52.6853,  12.7539,  33.3660,  65.1099, -36.1047,\n",
       "          41.1652, -29.1102,   8.9967],\n",
       "        [ 43.5203,  44.1402,  55.9121,  17.1286,  34.9502,  70.4931, -35.9536,\n",
       "          35.6416, -28.5321,  12.1842],\n",
       "        [ 37.3506,  46.3114,  51.6953,  14.4644,  34.9085,  68.2118, -37.1745,\n",
       "          38.4139, -32.6094,   9.5347]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EquivariantViT(n_rotations=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -5.2963,  -4.4130,  24.1830,  19.6565,  -4.5242,   0.1885,   0.7335,\n",
       "         -12.1216,  20.6063, -46.3707],\n",
       "        [ -5.3243,  -3.4945,  21.3550,  18.0383,  -5.8006,  -0.6816,  -0.1165,\n",
       "         -11.7457,  19.1885, -47.7863],\n",
       "        [ -5.2606,  -4.4213,  24.2381,  19.6619,  -4.5029,   0.1828,   0.6442,\n",
       "         -12.1724,  20.6091, -46.3625],\n",
       "        [ -5.3128,  -3.4906,  21.3652,  18.0476,  -5.8155,  -0.6852,  -0.1165,\n",
       "         -11.7535,  19.1261, -47.7776],\n",
       "        [ -5.3017,  -4.4111,  24.1761,  19.6539,  -4.5245,   0.1856,   0.7149,\n",
       "         -12.0858,  20.6099, -46.3162],\n",
       "        [ -5.3103,  -3.4895,  21.3567,  18.0471,  -5.8096,  -0.6905,  -0.1123,\n",
       "         -11.7461,  19.1613, -47.7241],\n",
       "        [ -5.3368,  -4.4045,  24.1211,  19.6481,  -4.5456,   0.1920,   0.8062,\n",
       "         -12.0287,  20.6096, -46.3183],\n",
       "        [ -5.3231,  -3.4937,  21.3463,  18.0368,  -5.7955,  -0.6865,  -0.1129,\n",
       "         -11.7390,  19.2220, -47.7317]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model(get_transforms(torch.randn((1, 1, 28, 28)), n_rotations=8, flips=False).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-57.7284, -14.8040,  45.7791, -59.9496, -27.1457,  12.3237,   8.8680,\n",
       "          96.6409,   9.8924, -76.2570],\n",
       "        [-55.4390, -16.7818,  48.2747, -57.6844, -25.0714,  11.1756,   7.9872,\n",
       "          91.1289,   7.6447, -75.3762],\n",
       "        [-63.3238, -19.3804,  58.6454, -69.4326, -28.6111,  10.5632,   3.4687,\n",
       "         102.9273,  11.9772, -87.4186],\n",
       "        [-65.4590, -19.8870,  51.3665, -65.6245, -32.2740,  12.9232,   8.0115,\n",
       "         103.0499,  13.4209, -84.9431]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gevit.eval()\n",
    "gevit(get_transforms(torch.randn((1, 1, 7, 7)), n_rotations=4, flips=False).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9370e-01,  1.4722e-01,  1.0389e-01,  6.5286e-02,  6.8986e-01,\n",
       "          1.3243e-01, -1.5173e-01, -3.2953e-01,  5.7994e-01, -1.1655e-02],\n",
       "        [ 1.8799e+00,  1.7593e-01, -1.3142e-01,  2.8236e-01,  2.5801e+00,\n",
       "         -3.2251e+00, -5.0657e-01, -1.3560e+00,  1.3613e+00, -7.9269e-01],\n",
       "        [-3.2089e-03,  1.4081e-01,  1.8851e-02,  6.0775e-02,  4.8662e-01,\n",
       "          1.8629e-01, -3.5152e-02, -2.4624e-01,  3.8574e-01, -3.6133e-02],\n",
       "        [ 1.7409e+00, -2.9496e-02, -3.4795e-02,  4.4164e-01,  2.7304e+00,\n",
       "         -2.2282e+00, -6.4203e-01, -1.2745e+00,  1.3482e+00, -4.6910e-01]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_transformer.eval()\n",
    "model.eval()\n",
    "model(n_data.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4115,\n",
       "          0.0085, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.3860, -0.1951,  1.2686,\n",
       "          0.0085,  1.3705, -0.1187, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.3733,  0.7468,  0.3777,  1.8923,  2.6560,\n",
       "          2.7451,  1.6505, -0.1187, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.1187, -0.3988,  2.0960,  2.0960,  2.7706,  2.6306,  2.6560,\n",
       "          1.2559, -0.0296, -0.1824, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "          0.6959,  0.6959,  2.4269,  2.7833,  2.7960,  2.7578,  2.6942, -0.1315,\n",
       "         -0.4115, -0.0296, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.9250,\n",
       "          2.7833,  2.7197,  2.7833,  2.7833,  2.7833, -0.4242, -0.4115, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.5542,  2.0196,  2.7578,\n",
       "          2.7578,  2.5669,  2.5542,  2.7960,  2.7578,  1.9178, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242,  2.5033,  2.5033,  2.7833,  1.8923,\n",
       "          2.3633, -0.3478,  0.3268,  2.3505,  2.7578,  0.9250,  0.7468, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242,  1.7269,  2.3124,  2.7833,  0.7468,  0.7468,\n",
       "         -0.3606, -0.4242, -0.3351,  0.7977,  2.7197,  2.6942, -0.3988, -0.4242,\n",
       "         -0.4115, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242,  0.2504,  2.5160,  2.5160,  0.8486,  0.7086, -0.3733,\n",
       "         -0.4242, -0.4242, -0.3988,  0.9759,  2.7833,  2.7833,  2.3251,  0.9504,\n",
       "         -0.4115, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.3224,  0.5686,  0.8486, -0.2460, -0.2842, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4115,  0.9759,  2.7706,  2.7706,  2.7833,\n",
       "          0.9632, -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.3860, -0.3860, -0.4115, -0.2460, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4115, -0.4242,  0.9632,  2.7706,  2.7833,\n",
       "          0.9886,  2.0323, -0.3478, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.9504,  2.2105,  2.7706,\n",
       "          2.7960,  2.7833, -0.2333, -0.0933, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0085,  2.8088,  2.7960,\n",
       "          2.7833,  2.7833,  2.7833,  0.3522,  0.3522,  0.0976, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.3606,  1.0141,  2.6942,  2.7960,\n",
       "          2.7324,  2.7324,  2.6815,  2.7833,  1.8032,  1.1668,  1.1668, -0.1696,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.2233,  2.7578,  0.0849,\n",
       "          2.0705,  1.8414,  1.6505,  1.6505,  2.4142,  2.7960,  1.5741, -0.1187,\n",
       "         -0.1187, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0467, -0.0296, -0.4115,\n",
       "         -0.4115, -0.4242, -0.1187,  0.9632,  2.2869,  2.2869,  2.7833,  1.9560,\n",
       "         -0.2715, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4115,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242,  0.6195,  2.1596,  2.6051,  2.6051,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0340,  2.0451, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "         -0.4242, -0.4242, -0.4242, -0.4242]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data.squeeze()[1,:,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF.rotate(torch.randn(1, 1,  28, 28), 90).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivariantViT(nn.Module):\n",
    "    def __init__(self, patch_size=7, num_patches=16, num_channels=1, n_rotations=4, flips=True, n_embd=1):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels  \n",
    "        self.n_rotations = n_rotations\n",
    "        self.flips = flips\n",
    "        self.n_embd = n_embd\n",
    "        self.num_patches_x = int(math.sqrt(num_patches))\n",
    "        # below can be more intricate, but for now we just use a linear layer\n",
    "        self.project = nn.Linear(num_channels*patch_size**2, n_embd)  # to project the patches to their embedding space\n",
    "        self.gevit = models.GroupTransformer(\n",
    "            group=groups.SE2(num_elements=8),\n",
    "            in_channels=1,\n",
    "            num_channels=20,\n",
    "            block_sizes=[2, 3],\n",
    "            expansion_per_block=1,\n",
    "            crop_per_layer=[0, 0, 0, 0, 0],\n",
    "            image_size=self.num_patches_x,\n",
    "            num_classes=10,\n",
    "            dropout_rate_after_maxpooling=0.0,\n",
    "            maxpool_after_last_block=False,\n",
    "            normalize_between_layers=False,\n",
    "            patch_size=None,\n",
    "            num_heads=9,\n",
    "            norm_type=\"LayerNorm\",\n",
    "            activation_function=\"Swish\",\n",
    "            attention_dropout_rate=0.0,\n",
    "            value_dropout_rate=0.01,\n",
    "            whitening_scale=1.41421356,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the patches\n",
    "        x = img_to_patch(x, self.patch_size, flatten_channels=False)  # B, num_patches, C, patch_size, patch_size\n",
    "        B, num_patches, C, patch_size, _ = x.shape\n",
    "\n",
    "        # get all transformations for the patches\n",
    "        x = x.view(B*num_patches, C, patch_size, patch_size)\n",
    "        x = get_transforms(x, n_rotations=self.n_rotations, flips=self.flips)\n",
    "\n",
    "        T = x.shape[1]  # number of transformations\n",
    "\n",
    "        # flatten and project all patches\n",
    "        x = x.view(B*num_patches*T, C*patch_size*patch_size)\n",
    "        x = self.project(x)\n",
    "\n",
    "        # combine the transformations for the patches to make it invariant\n",
    "        x = x.view(B, num_patches, T, self.n_embd)  # TODO check this\n",
    "        x = x.mean(dim=2)\n",
    "        \n",
    "        # reshape to image grid\n",
    "        x = x.view(B, self.num_patches_x, self.num_patches_x, self.n_embd).permute(0, 3, 1, 2)\n",
    "\n",
    "        # print(x.shape)\n",
    "        # pass through the GEViT to get predictions\n",
    "        x = self.gevit(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EquivariantViT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_model_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         model\u001b[38;5;241m.\u001b[39mload_state_dict(best_model_state)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, n_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):\n\u001b[1;32m     11\u001b[0m     epoch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# images = images.to(device)\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# targets = targets.to(device)\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/context.py:281\u001b[0m, in \u001b[0;36mForkProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_fork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl2023/lib/python3.11/multiprocessing/popen_fork.py:66\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m parent_r, child_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[1;32m     65\u001b[0m child_r, parent_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "def train(model, n_epochs=5):\n",
    "    # model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # best_val_acc = evaluate(model)\n",
    "    print(type(model).__name__)\n",
    "    # print(f\"Starting validaitons accuracy: {best_val_acc}\")\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        epoch_losses = []\n",
    "        for images, targets in train_loader:\n",
    "            # images = images.to(device)\n",
    "            # targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            print(loss.item())\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        # validate and store best model state\n",
    "        # val_acc = evaluate(model)\n",
    "        # if val_acc > best_val_acc:\n",
    "        #     best_val_acc = val_acc\n",
    "        #     best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # log epoch loss\n",
    "        # print(f\"Epoch {epoch+1}: loss {sum(epoch_losses)/len(epoch_losses):.4f}, validation accuracy {val_acc}\")\n",
    "\n",
    "    # Load best model state into the original model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
