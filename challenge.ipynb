{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from datasets import MNIST_rot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import models\n",
    "import torch\n",
    "import g_selfatt.groups as groups\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = (0.1307,)\n",
    "data_stddev = (0.3081,)\n",
    "transform_train = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(data_mean, data_stddev),\n",
    "    ]\n",
    ")\n",
    "transform_test = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(data_mean, data_stddev),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = MNIST_rot(root=\"./data\", stage=\"train\", download=True, transform=transform_train, data_fraction=1)\n",
    "evaluation_set = MNIST_rot(root=\"./data\", stage=\"evaluation\", download=True, transform=transform_train, data_fraction=1)\n",
    "test_set = MNIST_rot(root=\"./data\", stage=\"test\", download=True, transform=transform_test, data_fraction=1)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "evaluation_loader = torch.utils.data.DataLoader(\n",
    "    evaluation_set,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_loader), len(training_set)  # batch is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualize some samples from the training set\n",
    "num_samples_to_visualize = 4\n",
    "fig, axes = plt.subplots(1, num_samples_to_visualize, figsize=(12, 3))\n",
    "for i in range(num_samples_to_visualize):\n",
    "    image, label = training_set[i]\n",
    "    axes[i].imshow(np.squeeze(image), cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Visualize a batch of images loaded using the training loader\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Plot the images in the batch\n",
    "plt.figure(figsize=(10, 4))\n",
    "for idx in range(images.size(0)):\n",
    "    plt.subplot(1, 4, idx + 1)\n",
    "    plt.imshow(images[idx].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Label: {labels[idx].item()}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate some little training procedure to investigate speed of different parts of the codebase\n",
    "model = models.GroupTransformer(\n",
    "            group=groups.SE2(num_elements=4),\n",
    "            in_channels=1,\n",
    "            num_channels=20,\n",
    "            block_sizes=[2, 3],\n",
    "            expansion_per_block=1,\n",
    "            crop_per_layer=[2, 0, 2, 1, 1],\n",
    "            image_size=28,\n",
    "            num_classes=10,\n",
    "            dropout_rate_after_maxpooling=0.0,\n",
    "            maxpool_after_last_block=False,\n",
    "            normalize_between_layers=False,\n",
    "            patch_size=5,\n",
    "            num_heads=9,\n",
    "            norm_type=\"LayerNorm\",\n",
    "            activation_function=\"Swish\",\n",
    "            attention_dropout_rate=0.1,\n",
    "            value_dropout_rate=0.1,\n",
    "            whitening_scale=1.41421356,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try lr = 0.1, this is a really small model!!\n",
    "# also who uses swish, lets also try relu later (or does this break equivariance?)\n",
    "# it should be possible to train in less than 5 epochs\n",
    "# i set epochs to 4 so be patient with judging the loss\n",
    "# target loss: 0.2 (with normal run achieved after 50 epochs) \n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for inputs, labels in training_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "\n",
    "# first run with lr = 0.1 doesn't seem to be too high\n",
    "# might even try higher, also don't forget to experiment with relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = total = 0\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in training_loader[:len(training_loader//4)]:\n",
    "    out = model(inputs)\n",
    "    _, preds = torch.max(out, 1)\n",
    "    correct += (preds == labels).sum().item()\n",
    "    total += labels.size(0)\n",
    "\n",
    "correct / total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
