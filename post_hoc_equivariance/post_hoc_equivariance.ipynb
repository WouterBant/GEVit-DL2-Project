{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvzxovzoQOce",
        "outputId": "44ddea7d-bdb6-4d52-80a4-a343d0698ec9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/WouterBant/GEVit-DL2-Project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1RwgcuRQSJa",
        "outputId": "671d2f5b-627a-4c43-b461-9acb98d8e534"
      },
      "outputs": [],
      "source": [
        "%cd GEVit-DL2-Project/post_hoc_equivariance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95PQnE75QLKM"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JEti1RiQk2j",
        "outputId": "7c1e1118-a415-484c-ac56-30ea6519e806"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qTVlksnQLKN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as tvtf\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from datasets import MNIST_rot\n",
        "from train_vit import VisionTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPxJrqIXQLKO",
        "outputId": "55ba545a-6fb3-4d8b-9d11-094142b62b15"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if using CUDA\n",
        "    torch.backends.cudnn.deterministic = True  # if using CUDA\n",
        "    torch.backends.cudnn.benchmark = False  # if using CUDA, may improve performance but can lead to non-reproducible results\n",
        "\n",
        "def get_non_equivariant_vit():\n",
        "    model = VisionTransformer(embed_dim=64,\n",
        "                            hidden_dim=512,\n",
        "                            num_heads=4,\n",
        "                            num_layers=6,\n",
        "                            patch_size=4,\n",
        "                            num_channels=1,\n",
        "                            num_patches=49,\n",
        "                            num_classes=10,\n",
        "                            dropout=0.1).to(device)\n",
        "    model_path = \"../saved/results/model.pt\"\n",
        "    print(model.load_state_dict(torch.load(model_path, map_location=device), strict=False))\n",
        "    return model\n",
        "\n",
        "model = get_non_equivariant_vit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGv5nv01QLKO",
        "outputId": "98383faa-3485-4439-a034-517277988b20"
      },
      "outputs": [],
      "source": [
        "data_mean = (0.1307,)\n",
        "data_stddev = (0.3081,)\n",
        "\n",
        "transform_train = tvtf.Compose([\n",
        "    tvtf.RandomRotation(degrees=(-180, 180)),  # random rotation\n",
        "    tvtf.RandomHorizontalFlip(),  # random horizontal flip with a probability of 0.5\n",
        "    tvtf.RandomVerticalFlip(),\n",
        "    tvtf.ToTensor(),\n",
        "    tvtf.Normalize(data_mean, data_stddev)\n",
        "])\n",
        "transform_test = tvtf.Compose(\n",
        "    [\n",
        "        tvtf.ToTensor(),\n",
        "        tvtf.Normalize(data_mean, data_stddev),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_set = MNIST_rot(root=\"../data\", stage=\"train\", download=True, transform=transform_train, data_fraction=1, only_3_and_8=False)\n",
        "validation_set = MNIST_rot(root=\"../data\", stage=\"validation\", download=True, transform=transform_test, data_fraction=1, only_3_and_8=False)\n",
        "test_set = MNIST_rot(root=\"../data\", stage=\"test\", download=True, transform=transform_test, data_fraction=1, only_3_and_8=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    validation_set,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        ")\n",
        "img_loader = torch.utils.data.DataLoader(  # single element for visualization purposes\n",
        "    test_set,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DToN2C4cQLKO"
      },
      "outputs": [],
      "source": [
        "def train(model, n_epochs=5):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        epoch_losses = []\n",
        "        for images, targets in train_loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "        print(f\"Epoch {epoch+1}: loss {sum(epoch_losses)/len(epoch_losses):.4f}\")\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():  # disable gradient calculation during inference\n",
        "        for inputs, labels in tqdm(val_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # move inputs and labels to device\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    test_acc = 100 * correct / total\n",
        "    return test_acc\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():  # disable gradient calculation during inference\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # move inputs and labels to device\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    test_acc = 100 * correct / total\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "P5SJRZpTQLKO",
        "outputId": "7b5d65d6-fb86-41e9-f411-97ccf2e001d3"
      },
      "outputs": [],
      "source": [
        "data = iter(img_loader)\n",
        "image, target = next(data)\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "38iNQp65QLKP",
        "outputId": "e330fa75-c5c9-4ce7-fbd5-4981698aafd1"
      },
      "outputs": [],
      "source": [
        "def get_transforms(image, n_rotations=4, flips=True):\n",
        "    \"\"\"\n",
        "    Returns all transformations of a single input image\n",
        "    \"\"\"\n",
        "    transforms = [image]\n",
        "\n",
        "    # Rotations\n",
        "    for i in range(1, n_rotations):\n",
        "        angle = i * (360 / n_rotations)\n",
        "        rotated_image = TF.rotate(image, angle)\n",
        "        transforms.append(rotated_image)\n",
        "\n",
        "    # Flips\n",
        "    if flips:\n",
        "        flips = []\n",
        "        for transform in transforms:\n",
        "            flipped_image_lr = TF.hflip(transform)\n",
        "            flips.append(flipped_image_lr)\n",
        "        # for transform in transforms:\n",
        "        #     flipped_image_ud = TF.vflip(transform)\n",
        "        #     flips.append(flipped_image_ud)\n",
        "        transforms.extend(flips)\n",
        "\n",
        "    res = torch.cat(transforms)\n",
        "    return res\n",
        "\n",
        "def visualize_transforms(transformed_images):\n",
        "    num_images = len(transformed_images)\n",
        "    num_rows = (num_images - 1) // 4 + 1\n",
        "    fig, axes = plt.subplots(num_rows, 4, figsize=(16, 4*num_rows))\n",
        "\n",
        "    for i, img in enumerate(transformed_images):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        axes[row, col].imshow(img.permute(1, 2, 0), cmap=\"gray\")  # Permute dimensions for visualization\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    # Hide empty subplots\n",
        "    for i in range(num_images, num_rows * 4):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    # Labeling axes\n",
        "    for ax, col in zip(axes[0], ['Original', '90°', '180°', '270°']):\n",
        "        ax.set_title(col, size=\"larger\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "transformed_images = get_transforms(image)\n",
        "visualize_transforms(transformed_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF2IaohPQLKP",
        "outputId": "0f0f0a50-1479-4394-fe05-2da2b5a002ff"
      },
      "outputs": [],
      "source": [
        "model.forward(image.to(device), output_cls=True).shape, model.forward(get_transforms(image.to(device)), output_cls=True).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8JHKRbrQLKQ"
      },
      "source": [
        "#### Some possibilities of equivariant combination of the latent representations:\n",
        "- Mean pooling\n",
        "- Max pooling\n",
        "- Sum\n",
        "- Most probable\n",
        "- Highest probability among transformations\n",
        "- Learn weights for weighted average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from post_hoc_equivariant import *\n",
        "from sub_models import ScoringModel, Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First keeping the original model frozen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmasS5kmQLKR",
        "outputId": "40d16c36-497b-4ef8-8f07-ac2d5a2be231"
      },
      "outputs": [],
      "source": [
        "# baseline\n",
        "evaluate(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BroB1oQ9QLKR",
        "outputId": "8dd4b08b-a400-4040-8de9-34ca935b111a"
      },
      "outputs": [],
      "source": [
        "# mean pooling\n",
        "eq_model_mean = PostHocEquivariantMean(model)\n",
        "evaluate(eq_model_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrey97VoQLKR",
        "outputId": "86b8fef9-50d3-4aef-ec85-c566f1b19986"
      },
      "outputs": [],
      "source": [
        "# max pooling\n",
        "eq_model_max = PostHocEquivariantMax(model)\n",
        "evaluate(eq_model_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njN2WY_sQLKR",
        "outputId": "8334778e-47be-41af-97ce-002d32dcf7f6"
      },
      "outputs": [],
      "source": [
        "# summing latent dimensions\n",
        "eq_model_sum = PostHocEquivariantSum(model)\n",
        "evaluate(eq_model_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xB-DSBzQLKR",
        "outputId": "f5994c46-a106-481b-8cdb-2248787b0e6f"
      },
      "outputs": [],
      "source": [
        "# product of class probabilities\n",
        "eq_model_most_probable = PostHocEquivariantMostProbable(model)\n",
        "evaluate(eq_model_most_probable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSUVG_D3QLKR",
        "outputId": "33fed19d-8f49-4685-ce75-3f5473b056bf"
      },
      "outputs": [],
      "source": [
        "# take transformation with highest certainty for class\n",
        "eq_model_most_certain = PostHocMostCertain(model)\n",
        "evaluate(eq_model_most_certain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhGyAgetQLKR"
      },
      "source": [
        "#### Learn weighs for weighted average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itNygFTTQLKS"
      },
      "source": [
        "Also here there are a couple of options.\n",
        "- a) nn takes as input entire latent dimension and outputs scalar representing weight for that dimensions\n",
        "- b) nn takes as input the i'th entry of each latent dimension, the concatenation is a new latent dimension. Now we have to satisfy equivariance so this order of input should not matter. Use a transformer without PE in this case is a solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu8isgLzQLKS",
        "outputId": "e2d51721-91ec-4a66-de26-b66c7902fe42"
      },
      "outputs": [],
      "source": [
        "# a)\n",
        "set_seed(42)\n",
        "scoring_model = ScoringModel()\n",
        "eq_model_learned_score_aggregation = PostHocLearnedScoreAggregation(model=model, scoring_model=scoring_model)\n",
        "train(eq_model_learned_score_aggregation, n_epochs=10)\n",
        "evaluate(eq_model_learned_score_aggregation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sudn05wyQLKS",
        "outputId": "66ad1dfd-1fa6-4c77-9948-bf4727d61259"
      },
      "outputs": [],
      "source": [
        "# b)\n",
        "set_seed(42)\n",
        "aggregation_model = Transformer(embed_dim=64, hidden_dim=128, num_heads=4, num_layers=2, dropout=0.1)\n",
        "eq_model_learned_aggregation = PostHocLearnedAggregation(model=model, aggregation_model=aggregation_model)\n",
        "train(eq_model_learned_aggregation, n_epochs=25)\n",
        "evaluate(eq_model_learned_aggregation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Now all options but with finetuning the mlp_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mean pooling\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_mean = PostHocEquivariantMean(model, finetune_mlp_head=True)\n",
        "train(eq_model_mean, n_epochs=25)\n",
        "evaluate(eq_model_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max pooling\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_max = PostHocEquivariantMax(model, finetune_mlp_head=True)\n",
        "train(eq_model_max, n_epochs=25)\n",
        "evaluate(eq_model_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summing latent dimensions\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_sum = PostHocEquivariantSum(model, finetune_mlp_head=True)\n",
        "train(eq_model_sum, n_epochs=25)\n",
        "evaluate(eq_model_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# product of class probabilities\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_most_probable = PostHocEquivariantMostProbable(model, finetune_mlp_head=True)\n",
        "train(eq_model_most_probable, n_epochs=25)\n",
        "evaluate(eq_model_most_probable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# take transformation with highest certainty for class\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_most_certain = PostHocMostCertain(model, finetune_mlp_head=True)\n",
        "train(eq_model_most_certain, n_epochs=25)\n",
        "evaluate(eq_model_most_certain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a)\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "scoring_model = ScoringModel()\n",
        "eq_model_learned_score_aggregation = PostHocLearnedScoreAggregation(model=model, scoring_model=scoring_model, finetune_mlp_head=True)\n",
        "train(eq_model_learned_score_aggregation, n_epochs=25)\n",
        "evaluate(eq_model_learned_score_aggregation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# b)\n",
        "set_seed(42)\n",
        "aggregation_model = Transformer(embed_dim=64, hidden_dim=128, num_heads=4, num_layers=2, dropout=0.1)\n",
        "eq_model_learned_aggregation = PostHocLearnedAggregation(model=model, aggregation_model=aggregation_model, finetune_mlp_head=True)\n",
        "train(eq_model_learned_aggregation, n_epochs=25)\n",
        "evaluate(eq_model_learned_aggregation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Now all options but finetuning the entire base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mean pooling\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_mean = PostHocEquivariantMean(model, finetune_model=True)\n",
        "train(eq_model_mean, n_epochs=25)\n",
        "evaluate(eq_model_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max pooling\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_max = PostHocEquivariantMax(model, finetune_model=True)\n",
        "train(eq_model_max, n_epochs=25)\n",
        "evaluate(eq_model_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summing latent dimensions\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_sum = PostHocEquivariantSum(model, finetune_model=True)\n",
        "train(eq_model_sum, n_epochs=25)\n",
        "evaluate(eq_model_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# product of class probabilities\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_most_probable = PostHocEquivariantMostProbable(model, finetune_model=True)\n",
        "train(eq_model_most_probable, n_epochs=25)\n",
        "evaluate(eq_model_most_probable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# take transformation with highest certainty for class\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "eq_model_most_certain = PostHocMostCertain(model, finetune_model=True)\n",
        "train(eq_model_most_certain, n_epochs=25)\n",
        "evaluate(eq_model_most_certain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a)\n",
        "set_seed(42)\n",
        "model = get_non_equivariant_vit()\n",
        "scoring_model = ScoringModel()\n",
        "eq_model_learned_score_aggregation = PostHocLearnedScoreAggregation(model=model, scoring_model=scoring_model, finetune_model=True)\n",
        "train(eq_model_learned_score_aggregation, n_epochs=25)\n",
        "evaluate(eq_model_learned_score_aggregation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# b)\n",
        "set_seed(42)\n",
        "aggregation_model = Transformer(embed_dim=64, hidden_dim=128, num_heads=4, num_layers=2, dropout=0.1)\n",
        "eq_model_learned_aggregation = PostHocLearnedAggregation(model=model, aggregation_model=aggregation_model, finetune_model=True)\n",
        "train(eq_model_learned_aggregation, n_epochs=25)\n",
        "evaluate(eq_model_learned_aggregation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
