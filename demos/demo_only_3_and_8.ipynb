{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations for GE-ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import models\n",
    "from datasets import MNIST_rot\n",
    "import g_selfatt.groups as groups\n",
    "from g_selfatt.utils import num_params\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.GroupTransformer(\n",
    "    group=groups.E2(num_elements=8),\n",
    "    in_channels=1,\n",
    "    num_channels=20,\n",
    "    block_sizes=[2, 3],\n",
    "    expansion_per_block=1,\n",
    "    crop_per_layer=[2, 0, 2, 1, 1],\n",
    "    image_size=28,\n",
    "    num_classes=2,\n",
    "    dropout_rate_after_maxpooling=0.0,\n",
    "    maxpool_after_last_block=False,\n",
    "    normalize_between_layers=False,\n",
    "    patch_size=5,\n",
    "    num_heads=9,\n",
    "    norm_type=\"LayerNorm\",\n",
    "    activation_function=\"Swish\",\n",
    "    attention_dropout_rate=0.1,\n",
    "    value_dropout_rate=0.1,\n",
    "    whitening_scale=1.41421356,\n",
    ")\n",
    "model_path = \"checkpoints/rotMNIST_model_p4msa_type_Local_patch_5_dpatt_0.1_dpval_0.1_activ_Swish_norm_LayerNorm_white_1.41421356_optim_Adam_lr_0.001_bs_16_ep_50_wd_0.0001_seed_0_sched_constants_schdec_1.0.pt\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = (0.1307,)\n",
    "data_stddev = (0.3081,)\n",
    "transform_test = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(data_mean, data_stddev),\n",
    "    ]\n",
    ")\n",
    "test_set = MNIST_rot(root=\"../data\", stage=\"test\", download=True, transform=transform_test, data_fraction=1, only_3_and_8=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising latent representations of input throughout the model to test equivariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional\n",
    "\n",
    "\n",
    "idx_2_target = {\n",
    "    0: 3,\n",
    "    1: 8\n",
    "}\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for gif_ctr in range(1):\n",
    "        example_image, example_idx = next(data)\n",
    "        pil_image = Image.fromarray(example_image[0,0].numpy())\n",
    "        bg_color = example_image[0,0,0,0]\n",
    "        rotated_image = pil_image.rotate(90, fillcolor=bg_color)\n",
    "        rotated_image_tensor = torchvision.transforms.functional.pil_to_tensor(rotated_image).unsqueeze(1)\n",
    "        rotated_image_2 = pil_image.rotate(45, fillcolor=bg_color)\n",
    "        rotated_image_2_tensor = torchvision.transforms.functional.pil_to_tensor(rotated_image_2).unsqueeze(1)\n",
    "        rotated_image_3 = pil_image.rotate(20, fillcolor=bg_color)\n",
    "        rotated_image_3_tensor = torchvision.transforms.functional.pil_to_tensor(rotated_image_3).unsqueeze(1)\n",
    "        flipped_image = ImageOps.mirror(pil_image)\n",
    "        flipped_image_tensor = torchvision.transforms.functional.pil_to_tensor(flipped_image).unsqueeze(1)\n",
    "        translated_image = pil_image.transform(pil_image.size, Image.AFFINE, (1, 0, 3, 0, 1, 3), fillcolor=bg_color) \n",
    "        # fillcolor is set assuming that top left of image is the background color \n",
    "        translated_image_tensor = torchvision.transforms.functional.pil_to_tensor(translated_image).unsqueeze(1)\n",
    "        print(f\"Target: {idx_2_target[example_idx.item()]}\")\n",
    "        img_ctr = 0\n",
    "        for image in [example_image, rotated_image_tensor, rotated_image_2_tensor, rotated_image_3_tensor,\n",
    "                      flipped_image_tensor, translated_image_tensor]:\n",
    "            out = model(image)\n",
    "            features = model(image, show_features=True)\n",
    "            image = image.squeeze()  # Batch dimension\n",
    "            _, preds = torch.max(out, 1)\n",
    "            print(f\"{idx_2_target[preds.item()]} with probability {torch.softmax(out, dim=1)[0][preds].item()}\")\n",
    "            print(features[0].shape) # [batch size, channels, group elements, x ,y]\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            n_plotted_group_elements = 4\n",
    "\n",
    "            for j in range(n_plotted_group_elements):\n",
    "                for i in range(len(features)):\n",
    "                    if i==0: \n",
    "                        plt.subplot(n_plotted_group_elements, len(features)+1, j*(len(features)+1) + 1)\n",
    "                        if j == 0: plt.title('$g_0, c_0$')\n",
    "                        if j == 1: plt.title('$g_0, c_1$')\n",
    "                        if j == 2: plt.title('$g_1, c_0$')\n",
    "                        if j == 3: plt.title('$g_1, c_1$')\n",
    "                        plt.imshow(image.cpu().numpy(), cmap=\"gray\")\n",
    "                        plt.axis('off')\n",
    "                    plt.subplot(n_plotted_group_elements, len(features)+1, j*(len(features)+1) + i+2)\n",
    "                    if j==0: plt.title(f'layer {i}')\n",
    "                    if j==0: plt.imshow(features[i][0,0,0])\n",
    "                    if j==1: plt.imshow(features[i][0,1,0])\n",
    "                    if j==2: plt.imshow(features[i][0,0,1])\n",
    "                    if j==3: plt.imshow(features[i][0,1,1])\n",
    "                    plt.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'gif_tmps/tmp{img_ctr}.png')\n",
    "            img_ctr += 1\n",
    "            plt.show()\n",
    "        print(\"++++++++++++++++++++++++++++++\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising attention values to test equivariance (requires a hacky alternative model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.GroupTransformer(\n",
    "    group=groups.E2(num_elements=8),\n",
    "    in_channels=1,\n",
    "    num_channels=20,\n",
    "    block_sizes=[2, 3],\n",
    "    expansion_per_block=1,\n",
    "    crop_per_layer=[2, 0, 2, 1, 1],\n",
    "    image_size=28,\n",
    "    num_classes=2,\n",
    "    dropout_rate_after_maxpooling=0.0,\n",
    "    maxpool_after_last_block=False,\n",
    "    normalize_between_layers=False,\n",
    "    patch_size=5,\n",
    "    num_heads=9,\n",
    "    norm_type=\"LayerNorm\",\n",
    "    activation_function=\"Swish\",\n",
    "    attention_dropout_rate=0.1,\n",
    "    value_dropout_rate=0.1,\n",
    "    whitening_scale=1.41421356,\n",
    "    return_attn_probs=True,\n",
    ")\n",
    "model_path = \"checkpoints/rotMNIST_model_p4msa_type_Local_patch_5_dpatt_0.1_dpval_0.1_activ_Swish_norm_LayerNorm_white_1.41421356_optim_Adam_lr_0.001_bs_16_ep_50_wd_0.0001_seed_0_sched_constants_schdec_1.0.pt\"\n",
    "device2 = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model2 = torch.nn.DataParallel(model2)\n",
    "model2.load_state_dict(torch.load(model_path, map_location=device2), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2_target = {\n",
    "    0: 3,\n",
    "    1: 8\n",
    "}\n",
    "\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    for img_ctr in range(5):\n",
    "        example_image, example_idx = next(data)\n",
    "        out, attn = model2(example_image)\n",
    "        # attn.shape = B, N_heads, group_el, im_width, im_height, local_neigh_w, local_neigh, h\n",
    "        plt.figure(figsize=(8,2))\n",
    "        plt.subplot(161)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Target: {idx_2_target[example_idx.item()]}\")\n",
    "        plt.imshow(example_image.cpu().squeeze().numpy(), cmap=\"gray\")\n",
    "        plt.subplot(162)\n",
    "        plt.axis('off')\n",
    "        plt.title('Center pixel')\n",
    "        plt.imshow(example_image.cpu().squeeze().numpy()[14-2:14+3,14-2:14+3], cmap='gray')\n",
    "        plt.subplot(163)\n",
    "        plt.axis('off')\n",
    "        plt.title('$MHA_0, g_0$')\n",
    "        attn_image = attn[0,0,0,14,14,:,:].cpu().numpy()\n",
    "        plt.imshow(attn_image)\n",
    "        plt.subplot(164)\n",
    "        plt.axis('off')\n",
    "        plt.title('$MHA_0, g_1$')\n",
    "        attn_image = attn[0,0,1,14,14,:,:].cpu().numpy()\n",
    "        plt.imshow(attn_image)\n",
    "        plt.subplot(165)\n",
    "        plt.axis('off')\n",
    "        plt.title('$MHA_1, g_0$')\n",
    "        attn_image = attn[0,1,0,14,14,:,:].cpu().numpy()\n",
    "        plt.imshow(attn_image)\n",
    "        plt.subplot(166)\n",
    "        plt.axis('off')\n",
    "        plt.title('$MHA_1, g_1$')\n",
    "        attn_image = attn[0,1,1,14,14,:,:].cpu().numpy()\n",
    "        plt.imshow(attn_image)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'gif_tmps/attention/tmp{img_ctr}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising embedding weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding weights (lifting layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.module\n",
    "weights = module.lifting_self_attention.row_embedding[0].weight.detach().cpu().numpy()\n",
    "\n",
    "plt.plot(weights.squeeze())\n",
    "plt.title('Visualization of row_embedding weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = module.lifting_self_attention.col_embedding[0].weight.detach().cpu().numpy()\n",
    "plt.plot(weights.squeeze())\n",
    "plt.title('Visualization of col_embedding weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding indices (lifting layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices = module.lifting_self_attention.row_indices\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplots_adjust(bottom=0.85)\n",
    "plt.suptitle('Row embedding indices',size=10)\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.title(f'$g_{i}$',size=8)\n",
    "    plt.imshow(row_indices[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('row_embd_idxs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indices = module.lifting_self_attention.col_indices\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplots_adjust(bottom=0.85)\n",
    "plt.suptitle('Column embedding indices',size=10)\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.title(f'$g_{i}$',size=8)\n",
    "    plt.imshow(col_indices[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('col_embd_idxs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding indices (transformer block 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices = module.transformer[0].attention[2].row_indices\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(row_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indices = module.transformer[0].attention[2].col_indices\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(col_indices[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final embedding (transformer block 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices = module.transformer[0].attention[2].row_indices\n",
    "\n",
    "print(row_indices.shape)\n",
    "\n",
    "final_row_embedding = module.transformer[0].attention[2].row_embedding(row_indices.view(-1, 1, 1, 1)).view(row_indices.shape + (-1,)).detach().numpy()\n",
    "\n",
    "print(final_row_embedding.shape)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_row_embedding[i,:,:,0])\n",
    "    plt.title(f'$g_{i}$')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Channel 0 of transformed row embedding')\n",
    "plt.tight_layout()\n",
    "plt.savefig('row_embd_final.png')\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_row_embedding[i,:,:,1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_row_embedding[i,:,:,2])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indices = module.transformer[0].attention[2].col_indices\n",
    "\n",
    "print(col_indices.shape)\n",
    "\n",
    "final_col_embedding = module.transformer[0].attention[2].col_embedding(col_indices.view(-1, 1, 1, 1)).view(col_indices.shape + (-1,)).detach().numpy()\n",
    "\n",
    "print(final_col_embedding.shape)\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_col_embedding[i,:,:,0])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_col_embedding[i,:,:,1])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_col_embedding[i,:,:,2])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to visualize combination of relative row and col embeddings\n",
    "final_row_embedding = module.transformer[0].attention[2].row_embedding(row_indices.view(-1, 1, 1, 1)).view(row_indices.shape + (-1,)).detach().numpy()\n",
    "final_col_embedding = module.transformer[0].attention[2].col_embedding(col_indices.view(-1, 1, 1, 1)).view(col_indices.shape + (-1,)).detach().numpy()\n",
    "\n",
    "row = final_row_embedding[0,:,:,0]\n",
    "col = final_col_embedding[0,:,:,0]\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Row embedding')\n",
    "plt.imshow(row)\n",
    "plt.axis('off')\n",
    "plt.subplot(132)\n",
    "plt.title('Column embedding')\n",
    "plt.imshow(col)\n",
    "plt.axis('off')\n",
    "plt.subplot(133)\n",
    "plt.title('Combined')\n",
    "plt.imshow(row+col)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('combined_embd.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final embedding (transformer block 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indices = module.transformer[3].attention[2].col_indices\n",
    "\n",
    "print(col_indices.shape)\n",
    "\n",
    "final_col_embedding = module.transformer[3].attention[2].col_embedding(col_indices.view(-1, 1, 1, 1)).view(col_indices.shape + (-1,)).detach().numpy()\n",
    "final_col_embedding = module.transformer[3].attention[2].col_embedding(col_indices.view(-1, 1, 1, 1)).view(col_indices.shape + (-1,)).detach().numpy()\n",
    "\n",
    "print(final_col_embedding.shape)\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_col_embedding[i,:,:,0])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_col_embedding[i,:,:,1])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_col_embedding[i,:,:,2])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What it comes down to:\n",
    "- These embeddings are for an area around a given pixel to embed the relative position to the center.\n",
    "- You can see that the relative row/col embeddings mostly work to distinguish relative position, but that it is group action'ed for every group element to stay equivariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distances between embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = [final_col_embedding[i] for i in range(8)]\n",
    "\n",
    "dists_abs = np.zeros((8,8))\n",
    "dists_euc = np.zeros((8,8))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        dists_abs[i][j] = np.sum(np.abs(embeds[i] - embeds[j]))\n",
    "        dists_euc[i][j] = np.sqrt(np.sum((embeds[i] - embeds[j])**2))\n",
    "\n",
    "plt.title('absolute distances between col embeddings (per group el)')\n",
    "plt.imshow(dists_abs)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.title('euclidean distances between col embeddings')\n",
    "plt.imshow(dists_euc)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = [final_row_embedding[i] for i in range(8)]\n",
    "\n",
    "dists_abs = np.zeros((8,8))\n",
    "dists_euc = np.zeros((8,8))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        dists_abs[i][j] = np.sum(np.abs(embeds[i] - embeds[j]))\n",
    "        dists_euc[i][j] = np.sqrt(np.sum((embeds[i] - embeds[j])**2))\n",
    "\n",
    "plt.title('absolute distances between col embeddings (per group el)')\n",
    "plt.imshow(dists_abs)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.title('euclidean distances between col embeddings')\n",
    "plt.imshow(dists_euc)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer block 0 embedding kernel weight visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_row = module.transformer[0].attention[2].row_embedding[0].weight.detach().cpu().numpy()\n",
    "weights_col = module.transformer[0].attention[2].col_embedding[0].weight.detach().cpu().numpy()\n",
    "\n",
    "plt.plot(weights_row.squeeze())\n",
    "plt.plot(weights_col.squeeze())\n",
    "\n",
    "print(weights_col.shape)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_row = module.transformer[0].attention[2].row_embedding[3].weight.detach().cpu().numpy()\n",
    "weights_col = module.transformer[0].attention[2].col_embedding[3].weight.detach().cpu().numpy()\n",
    "\n",
    "print(weights_row.squeeze.shape)\n",
    "plt.subplot(211)\n",
    "plt.plot(weights_col.squeeze().T)\n",
    "plt.subplot(212)\n",
    "plt.plot(weights_row.squeeze().T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_row = module.transformer[0].attention[2].row_embedding[0].weight.detach().cpu().numpy()\n",
    "weights_col = module.transformer[0].attention[2].col_embedding[0].weight.detach().cpu().numpy()\n",
    "\n",
    "print(weights_col.squeeze())\n",
    "\n",
    "combo = np.zeros((16, 16))\n",
    "\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        combo[i][j] = weights_row.squeeze()[i] + weights_col.squeeze()[j]\n",
    "\n",
    "plt.imshow(combo)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_row = module.transformer[0].attention[2].row_embedding[0].weight.detach().cpu().numpy()\n",
    "weights_col = module.transformer[0].attention[2].col_embedding[0].weight.detach().cpu().numpy()\n",
    "\n",
    "print(weights_row.squeeze().shape)\n",
    "\n",
    "\n",
    "combo = np.zeros((16, 16))\n",
    "\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        combo[i][j] = weights_row.squeeze()[i] * weights_col.squeeze()[j]\n",
    "\n",
    "plt.imshow(combo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untrained model tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.GroupTransformer(\n",
    "    group=groups.E2(num_elements=8),\n",
    "    in_channels=1,\n",
    "    num_channels=20,\n",
    "    block_sizes=[2, 3],\n",
    "    expansion_per_block=1,\n",
    "    crop_per_layer=[2, 0, 2, 1, 1],\n",
    "    image_size=28,\n",
    "    num_classes=2,\n",
    "    dropout_rate_after_maxpooling=0.0,\n",
    "    maxpool_after_last_block=False,\n",
    "    normalize_between_layers=False,\n",
    "    patch_size=5,\n",
    "    num_heads=9,\n",
    "    norm_type=\"LayerNorm\",\n",
    "    activation_function=\"Swish\",\n",
    "    attention_dropout_rate=0.1,\n",
    "    value_dropout_rate=0.1,\n",
    "    whitening_scale=1.41421356,\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional\n",
    "\n",
    "\n",
    "idx_2_target = {\n",
    "    0: 3,\n",
    "    1: 8\n",
    "}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(2):\n",
    "        example_image, example_idx = next(data)\n",
    "        pil_image = Image.fromarray(example_image[0,0].numpy())\n",
    "        bg_color = example_image[0,0,0,0]\n",
    "        rotated_image = pil_image.rotate(90, fillcolor=bg_color)\n",
    "        rotated_image_tensor = torchvision.transforms.functional.pil_to_tensor(rotated_image).unsqueeze(1)\n",
    "        rotated_image_2 = pil_image.rotate(45, fillcolor=bg_color)\n",
    "        rotated_image_2_tensor = torchvision.transforms.functional.pil_to_tensor(rotated_image_2).unsqueeze(1)\n",
    "        rotated_image_3 = pil_image.rotate(20, fillcolor=bg_color)\n",
    "        rotated_image_3_tensor = torchvision.transforms.functional.pil_to_tensor(rotated_image_3).unsqueeze(1)\n",
    "        flipped_image = ImageOps.mirror(pil_image)\n",
    "        flipped_image_tensor = torchvision.transforms.functional.pil_to_tensor(flipped_image).unsqueeze(1)\n",
    "        translated_image = pil_image.transform(pil_image.size, Image.AFFINE, (1, 0, 3, 0, 1, 3), fillcolor=bg_color) \n",
    "        # fillcolor is set assuming that top left of image is the background color \n",
    "        translated_image_tensor = torchvision.transforms.functional.pil_to_tensor(translated_image).unsqueeze(1)\n",
    "        print(f\"Target: {idx_2_target[example_idx.item()]}\")\n",
    "        for image in [example_image, rotated_image_tensor, rotated_image_2_tensor, rotated_image_3_tensor,\n",
    "                      flipped_image_tensor, translated_image_tensor]:\n",
    "            out = model(image)\n",
    "            features = model(image, show_features=True)\n",
    "            image = image.squeeze()  # Batch dimension\n",
    "            plt.imshow(image.cpu().numpy(), cmap=\"gray\")\n",
    "            plt.show()\n",
    "            _, preds = torch.max(out, 1)\n",
    "            print(f\"{idx_2_target[preds.item()]} with probability {torch.softmax(out, dim=1)[0][preds].item()}\")\n",
    "            print(features[0].shape) # [batch size, channels, group elements, x ,y]\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            n_plotted_group_elements = 2\n",
    "            for i in range(len(features)):\n",
    "                for j in range(n_plotted_group_elements):\n",
    "                    plt.subplot(n_plotted_group_elements, len(features), j*len(features) + i+1)\n",
    "                    plt.imshow(features[i][0,0,j])\n",
    "            plt.show()\n",
    "        print(\"++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices = module.transformer[0].attention[2].row_indices\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(row_indices[i])\n",
    "\n",
    "col_indices = module.transformer[0].attention[2].col_indices\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(col_indices[i])\n",
    "\n",
    "row_indices = module.transformer[0].attention[2].row_indices\n",
    "\n",
    "print(row_indices.shape)\n",
    "\n",
    "final_row_embedding = module.transformer[0].attention[2].row_embedding(row_indices.view(-1, 1, 1, 1)).view(row_indices.shape + (-1,)).detach().numpy()\n",
    "\n",
    "print(final_row_embedding.shape)\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_row_embedding[i,:,:,0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_row_embedding[i,:,:,1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_row_embedding[i,:,:,2])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "col_indices = module.transformer[0].attention[2].col_indices\n",
    "\n",
    "print(col_indices.shape)\n",
    "\n",
    "final_col_embedding = module.transformer[0].attention[2].col_embedding(col_indices.view(-1, 1, 1, 1)).view(col_indices.shape + (-1,)).detach().numpy()\n",
    "\n",
    "print(final_col_embedding.shape)\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_col_embedding[i,:,:,0])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_col_embedding[i,:,:,1])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(final_col_embedding[i,:,:,2])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Attempt to visualize combination of relative row and col embeddings\n",
    "final_row_embedding = module.transformer[0].attention[2].row_embedding(row_indices.view(-1, 1, 1, 1)).view(row_indices.shape + (-1,)).detach().numpy()\n",
    "final_col_embedding = module.transformer[0].attention[2].col_embedding(col_indices.view(-1, 1, 1, 1)).view(col_indices.shape + (-1,)).detach().numpy()\n",
    "\n",
    "row = final_row_embedding[0,:,:,0]\n",
    "col = final_col_embedding[0,:,:,0]\n",
    "\n",
    "plt.imshow(row)\n",
    "plt.show()\n",
    "plt.imshow(col)\n",
    "plt.show()\n",
    "plt.imshow(row+col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over named modules\n",
    "# for name, module in model.named_modules():\n",
    "    # print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo visualize attention and all the different layers\n",
    "# just a bunch of explainability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
