{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yPfj4x6Q74Z5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "\n",
        "import sys\n",
        "sys.path.append('../src/')\n",
        "from datasets import MNIST_rot\n",
        "from g_selfatt.utils import num_params\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jacE0BS774Z6",
        "outputId": "751c6ff7-b920-4de1-d3a2-1c609935456d"
      },
      "outputs": [],
      "source": [
        "data_mean = (0.1307,)\n",
        "data_stddev = (0.3081,)\n",
        "train_test = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=(-180, 180)),  # Random rotation between -15 to +15 degrees\n",
        "    transforms.RandomHorizontalFlip(),  # Random horizontal flip with a probability of 0.5\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(data_mean, data_stddev)\n",
        "])\n",
        "transform_test = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(data_mean, data_stddev),\n",
        "    ]\n",
        ")\n",
        "train_set = MNIST_rot(root=\"../data\", stage=\"train\", download=True, transform=train_test, data_fraction=1, only_3_and_8=False)\n",
        "test_set = MNIST_rot(root=\"../data\", stage=\"test\", download=True, transform=transform_test, data_fraction=1, only_3_and_8=False)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AAWEbPx74Z7"
      },
      "outputs": [],
      "source": [
        "# https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html\n",
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: Tensor representing the image of shape [B, C, H, W]\n",
        "        patch_size: Number of pixels per dimension of the patches (integer)\n",
        "        flatten_channels: If True, the patches will be returned in a flattened format\n",
        "                           as a feature vector instead of a image grid.\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    x = x.reshape(B, C, H // patch_size, patch_size, W // patch_size, patch_size)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5)  # [B, H', W', C, p_H, p_W]\n",
        "    x = x.flatten(1, 2)  # [B, H'*W', C, p_H, p_W]\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2, 4)  # [B, H'*W', C*p_H*p_W]\n",
        "    return x\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
        "        \"\"\"Attention Block.\n",
        "\n",
        "        Args:\n",
        "            embed_dim: Dimensionality of input and attention feature vectors\n",
        "            hidden_dim: Dimensionality of hidden layer in feed-forward network\n",
        "                         (usually 2-4x larger than embed_dim)\n",
        "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
        "            dropout: Amount of dropout to apply in the feed-forward network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        inp_x = self.layer_norm_1(x)\n",
        "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
        "        x = x + self.linear(self.layer_norm_2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        hidden_dim,\n",
        "        num_channels,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        num_classes,\n",
        "        patch_size,\n",
        "        num_patches,\n",
        "        dropout=0.0,\n",
        "    ):\n",
        "        \"\"\"Vision Transformer.\n",
        "\n",
        "        Args:\n",
        "            embed_dim: Dimensionality of the input feature vectors to the Transformer\n",
        "            hidden_dim: Dimensionality of the hidden layer in the feed-forward networks\n",
        "                         within the Transformer\n",
        "            num_channels: Number of channels of the input (3 for RGB)\n",
        "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
        "            num_layers: Number of layers to use in the Transformer\n",
        "            num_classes: Number of classes to predict\n",
        "            patch_size: Number of pixels that the patches have per dimension\n",
        "            num_patches: Maximum number of patches an image can have\n",
        "            dropout: Amount of dropout to apply in the feed-forward network and\n",
        "                      on the input encoding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Layers/Networks\n",
        "        self.input_layer = nn.Linear(num_channels * (patch_size**2), embed_dim)\n",
        "        self.transformer = nn.Sequential(\n",
        "            *(AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers))\n",
        "        )\n",
        "        self.mlp_head = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, num_classes))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Parameters/Embeddings\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
        "\n",
        "    def forward(self, x, output_cls=False):\n",
        "        # Preprocess input\n",
        "        x = img_to_patch(x, self.patch_size)\n",
        "        B, T, _ = x.shape\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        # Add CLS token and positional encoding\n",
        "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        x = x + self.pos_embedding[:, : T + 1]\n",
        "\n",
        "        # Apply Transforrmer\n",
        "        x = self.dropout(x)\n",
        "        x = x.transpose(0, 1)\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Perform classification prediction\n",
        "        cls = x[0]\n",
        "        if output_cls:\n",
        "            return cls\n",
        "\n",
        "        out = self.mlp_head(cls)\n",
        "        return out\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = VisionTransformer(embed_dim=64,\n",
        "                          hidden_dim=256,\n",
        "                          num_heads=4,\n",
        "                          num_layers=6,\n",
        "                          patch_size=4,\n",
        "                          num_channels=1,\n",
        "                          num_patches=49,\n",
        "                          num_classes=10,\n",
        "                          dropout=0.1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJzKLwKV74Z7"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), 0.0001)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
        "\n",
        "# scheduler for linear warmup of lr and then cosine decay\n",
        "# linear_warmup = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1/10, end_factor=1.0, total_iters=10-1, last_epoch=-1, verbose=True)\n",
        "# cos_decay = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=200-10, eta_min=1e-5, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71hFGlJI74Z7",
        "outputId": "0eae51e8-03a2-47aa-b944-386b91f3b587"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of parameters in the model: {num_params(model)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJmRYUAi74Z8",
        "outputId": "fbaa4e58-3ec1-41ff-c93a-f3665d1f2f72"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "for epoch in range(200):\n",
        "    losses = []\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to device\n",
        "        optimizer.zero_grad()\n",
        "        out = model(inputs)\n",
        "        loss = criterion(out, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()  # Update weights\n",
        "        losses.append(loss.item())\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {sum(losses)/len(losses)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYeDTZ0RAaYA"
      },
      "outputs": [],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():  # Disable gradient calculation during inference\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to device\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
